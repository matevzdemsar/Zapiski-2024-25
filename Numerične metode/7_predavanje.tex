\documentclass[a4paper]{article}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{esint}
\setlength{\parindent}{0em}
\setlength{\parskip}{1ex}
\newcommand{\vct}[1]{\overrightarrow{#1}}
\newcommand{\dif}{\mathrm{d}}
\newcommand{\pd}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\dd}[2]{\frac{\mathrm{d} {#1}}{\mathrm{d} {#2}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\fn}[3]{{#1}\colon {#2} \rightarrow {#3}}
\newcommand{\avg}[1]{\langle {#1} \rangle}
\newcommand{\Sum}[2][0]{\sum_{{#2} = {#1}}^{\infty}}
\newcommand{\Lim}[1]{\lim_{{#1} \rightarrow \infty}}
\newcommand{\Binom}[2]{\begin{pmatrix} {#1} \cr {#2} \end{pmatrix}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}

\begin{document}
\paragraph{Linearni problem najmanjših kvadratov (nadaljevanje).} Imamo razcep \(A = QR\), kjer je \(Q\in\R^{m \times m}\) ortogonalna matrika, \(R \in \R^{m \times n}\) pa zgornja trapezna matrika. \\[3mm]
Od prej imamo rešitev \[A^TA x = A^Tb\]
Ko vstavimo razcep, dobimo \[(QR)^T(QR)x = (QR)^Tb\]
\[R^T(Q^TQ)Rx = R^TQ^Tb\]
\[R^TRx = R^TQ^Tb\]
Tokrat ne moremo na obeh straneh množiti z \((R^T)^{-1}\), saj \(R^T\) ni kvadratna.
Ta postopek torej ni najprimernejši.
\paragraph{Alternativa.} Iščemo minimum \(||Ax-b||_2^2\)
\[\norm{Ax-b}_2^2 = \norm{QRx-b}_2^2 = \norm{Rx-Q^Tb}_2^2\]
Matriko \(R\) in vektor \(Q^Tb\) razdelimo na dva dela: prvih \(n\) vrstic in preostalih \(m-n\) vrstic Označimo še \(c = Q^Tb\).
Velja: \[\norm{Rx - Q^Tb}_2^2 = \norm{\begin{bmatrix}
    R_1x \\ 0
\end{bmatrix} - \begin{bmatrix}
    c_1 \\ c_2
\end{bmatrix}}_2^2 = \norm{\begin{bmatrix}
    R_1x - c_1 \\ -c
\end{bmatrix}}_2^2 = \norm{R_1x - c_1}_2^2 + \norm{c_2}_2^2\]
Upoštevali smo definicijo kvadratne norme. Ugotovimo, da bo izraz minimalen, ko bo \(R_1x = c_1\), kar je zgornje trikoten sistem, rešljiv z vstavljenjem.
\paragraph{Opomba.} Dokažemo lahko, da obstaja natanko en QR razcep, pri katerem ima \(R\) pozitivne diagonalne elemente.
\paragraph{Hausholderjeva zrcaljenja.} Gre za še eno metodo reševanja problema najmanjših kvadratov. Osnovana je na Givensovi rotaciji:
ideja je v tem, da vse elemente pod diagonalo z eno operacijo nastavimo na 0. \\[3mm]
Zamislimo si hiperravnino v \(\R^{m}\), na katero bodi vektor \(w\) za izbrani skalarni produkt pravokoten. Ta vektor lastnoročno določa hiperravnino.
Poskusimo napisati transformacijo, ki neki vektor \(x\) vzdolž vektorja \(w\) preslika preko hiperravnine.
\[Px = x - 2\alpha w = x - 2\frac{w^Tx}{w^Tw}w = x-\frac{2}{w^Tw}(ww^T)x = \left(I-\frac{2}{w^Tw}(w \otimes w)\right)x\]
\paragraph{Lastnosti zrcaljenja.} Velja:
\begin{align*}
    P & = P^T \\
    P^2 & = I \\
    PP^T = I \\
\end{align*}
Kako to uporabimo za QR razcep? Če s \(P_1\) označimo matriko preslikave vzdolž \(a_1\), s \(P_2\) matriko preslikave vzdolž \(a_2\) in tako naprej, je
\[A = QR = (P_1P_2P_3...P_n)R\]
\paragraph{Primerjava metod.}
\begin{itemize}
    \item Normalni sistem: \(mn^2\), vendar je nestabilen.
    \item Modificiran Gram-Schmidt: \(2mn^2\)
    \item Givens: \(3mn^2-n^3\)
    \item Householder: \(2mn^2-\frac{2}{3}n^3\), najbolj numerično stabilen
\end{itemize}
\paragraph{Singularni razcep.} Izrek: Za vsako matriko \(A \in \R^{m \times n},~m \geq n\) obstaja singularni razcep
\[A = U \Sigma V^T\]
Tu sta matriki \(U \in \R^{m \times m}\) in \(V \in \R^{n \times n}\) ortogonalni, \(\Sigma \in \R^{m \times n}\) pa kvazidiagonalna (trapezna) matrika
z diagonalnimi elementi \(\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_n > 0\)
\paragraph{Lema.} Če je \(A \in \R^{m \times n}\) matrika s polnim rangom (\(\mathrm{rang}(A) = n\)), je minimum \(\norm{Ax-b}_2\) dosežen pri vektorju
\[x = \sum_{j=1}^n\frac{u_j^Tb}{\sigma_j}v_j\]
Takšnega razcepa sicer ni zelo lahko izračunati in ga pri tem predmetu ne bomo podrobneje obravnavali.
\paragraph{Izrek.} Naj bo \(A = U \Sigma V^T\) singularni razcep matrike \(A \in \R^{m \times n},~m \geq n\), in naj velja \(\mathrm{rang}(A) > k < n\).
Označimo \(\displaystyle{A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T \in \R^{m \times n}}\) \\[3mm]
Potem je \[\min_{\mathrm{rang}(B)=k} \norm{B-A}_2 = \norm{A_k - A}_2 = \sigma_{k+1}\]
\[\min_{\mathrm{rang}(B)=k} \norm{B-A}_F = \norm{A_k - A}_F = \sqrt{\sum_{i=k+1}^{n}\sigma_i^2}\]
\paragraph{Problem lastnih vrednosti.} Imamo matriko \(A \in \R^{n \times n}\). Par vektorja in lastne vrednosti \(x, \lambda\), za katerega velja
\[Ax = \lambda x,\] imenujemo lastni par. Bolj specifično je \(\lambda\) lastna vrednost, \(x\) pa lastni vektor. \\
\(y\) je levi lastni vektor, če je \[y^H A = \lambda y^H\]
Dovolj je obravnavati samo desne lastne vektorje (levi lastni vektorji so desni lastni vektorji matrike \(A^H\)).
\paragraph{Lema.} Levi in desni lastni vektorji matrike \(A\), ki ji pripadajo različnim lastnim vrednostim, so ortogonalni.
\paragraph{Iskanje lastnih vrednosti.} Iskanje preko ničel karakterističnega polinoma numerično ni dober pristop. Poznamo pa stabilnejše algoritme:
Izbor algoritma je odvisen od tega, kaj želimo izračunati ter lastnosti matrike:
\begin{itemize}
    \item Ali je majhna in polna ali velika in razpršena?
    \item Ali je simetrična? (to nam precej olajša delo)
    \item Ali potrebujemo vse lastne vrednosti ali jih zadošča le nekaj?
    \item Ali potrebujemo tudi lastne vektorje?
\end{itemize} 
\paragraph{Izrek.} Za vsako matriko obstaja unitarna matrika \(U\) in zgornje trikotna matrika \(T\), da je \[U^H A U = T\]
Temu pravimo Schurrova forma, izrek smo že obravnavali pri Matematiki II.
\paragraph{Izrek.} Za vsako realno matriko \(A\) obstajata ortogonalna matriko \(Q\) in kvazi zgornje trikotna matrika \(T\),
da je \[Q^TAQ =T\]
Z izrazom kvazi zgornje trikotna tu povemo, da ima vzdolž diagonale \(2 \times 2\) rotacijske matrike.
\paragraph{Potenčna metoda.} S potenčno metodo dobimo lastno vrednost, ki je po lastni vrednosti največja. \\
Algoritem je sledeč: Izberemo normiran vektor \(z_0 \in \R^n\). Nato imamo dva koraka:
\[y_{k+1} = Az_k\]
\[z_{k+1} = \frac{1}{\norm{y_{k+1}}}y_{k+1}\]
Zaporedje konvergira proti lastnemu vektorju, razen včasih, ko z nadaljnjimi iteracijami le menja predznak. Če konvergira proti nekemu vektorju, ta pripada
po absolutno največji lastni vrednosti.
\paragraph{Izrek.} Naj bo \(\lambda_1\) dominantna (po absolutni vrednosti največja) lastna vrednost matrike. Če ima \(z_0\) neničelno komponento v smeri lastnega
vektorja, ki pripada \(\lambda_1\), potem zaporedje vektorjev \(z_k\) po smeri konvergira k lastnemu vektorju.
Kako zagotovimo, da bo imel \(z_0\) neničelno komponento v smeri lastnega vektorja? Tu nam je v pomoč numerična napaka pri računanju:
takega vektorja enostavno ne moremo zanesljivo doseči. \\
Metodo lahko priredimo za primera, ko sta največji lastni vrednosti po absolutni vrednosti enaki, dokler sta si med sabo nasprotno enaki (v realnih številih) ali
pa je ena lastna vrednost transponiranka druge (v kompleksnih številih). Če sta dve lastni vrednosti točno enaki ali pa se med sabo po absolutni vrednosti
ujemajo tri lastne vrednosti ali več, ta metoda ne deluje.
\paragraph{Ustavljalni pogoj.} Kdaj končamo iteracijo potenčne metode? Primerjava zaporednih približkov ni nujno ustrezna, saj sta lahko zaporedna približka nasprotno enaka,
kar nam da veliko razliko, hkrati pa sta lahko oba vzporedna z lastnim vektorjem. Tudi, če vzamemo več zaporednih približkov, je za naše namene bolj ustrezen Rayleighov koeficient:
\paragraph{Rayleighov kvocient.} Definiran je kot 
$$\rho (\mathbf{x}, A) = \frac{\mathbf{x}^HA\mathbf{x}}{\mathbf{x}^H\mathbf{x}}$$
Če je $\lambda, \mathbf{v}$ lastni par (lastna vrednost in pripadajoči lastni vektor matrike $A$), je
$$\rho (\mathbf{v}, A) = \frac{v^TAv}{v^Tv} = \frac{v^T\lambda v}{v^Tv} = \lambda$$
Če je $w \approx v$, je tudi $\rho (w, A) \approx \lambda$. Lastne vrednosti torej iščemo z iteracijo (če že poznamo lastni vektor),
ki jo ustavimo, ko je $||A\mathbf{z_k} - \rho(\mathbf{z_k}, A)\mathbf{z_k}|| \leq \varepsilon$ - ko dosežemo želeni nivo natančnosti. \\
Potenčna metoda ima linearno konvergenco.
Ko na ta način izračunamo dominantno lastno vrednost, poskusimo problem reducirati na naslednjo največjo lastno vrednost. \\
Privzeli bomo, da je $A$ simetrične, torej so njeni lastni vektorji lahko ortonormirani. Definiramo $B := A - \lambda_1x_1x^T_1$
Lastne vrednosti matrike $B$ so:
$$Bx_1 = Ax_1 - \lambda_1x_1||x_1||_2^2 = \lambda_1x_1 - \lambda_1x_1||x_1||^2 = 0 \left(\cdot x_1\right)$$
$$Bx_i = Ax_i - \lambda_1x_1(x_1^T \cdot x_i) = Ax_i = \lambda_ix_i$$
Uporabili smo dejstvo, da so lastni vektorji ortonormirani. Sledi, da ima $B$ iste lastne vrednosti kot $A$, razen $\lambda_1$.
Sledi, da bo dominantna lastna vrednost matrike $B$ neka druga lastna vrednost. \\
\paragraph{Splošne matrike.} Če $A$ ni simetrična, naredimo Householderjevo redukcijo. Poiščemo tako matriko $Q$, da je $Q\mathbf{x_1} = k\mathbf{\hat{e}_1}$.
Nato definiramo matriko $B$, ki je oblike $$B QAQ^T= \begin{bmatrix}
    \lambda_1 & \mathbf{b}^T \\
    0 & C
\end{bmatrix}$$
Zdaj moramo poiskati še lastne vrednosti matrike $C$, ki se ujemajo z lastnimi vrednostmi matrika \(A\) - problem torej rešujemo rekurzivno. Spet pa smo predpostavili, da je $\lambda$ dominantna lastna vrednost in da smo našli
pripadajoči lastni vektor $\mathbf{x_1}$. Čim to ni izpolnjeno, ne moremo iskati lastnih vrednosti.
\paragraph{Inverzna iteracija.} Najmanjša lastna vrednost matrike $A$ je hkrati največja lastna vrednost matrike $A^{-1}$. Sledi, da lahko namesto sistema enačb $$Ax_1 = \lambda_1x_1$$
rešujemo sistem enačb $$A^{-1}x_1 = \frac{1}{\lambda_1}x_1$$
Problem: Računanje inverza $A$ je v splošnem zoprno. Namesto tega lahko na obeh straneh množimo z $A$ in rešujemo sistem $$A\mathbf{y}_{k+1} = \mathbf{z}_k$$
Inverzna iteracija je uporabna, če poznamo dober približek lastno vrednost in iščemo lastni vektor. Stvar se nekoliko zakomplicira, če uporabimo točno lastno vrednost, a zaradi numeričnih
napak to običajno ni težava.
\paragraph{Računanje lastnih vektorjev.} Recimo, da poznamo lastno vrednost matrike \(A\) in želimo izračunati njen lastni vektor. To storimo tako, da izvedemo inverzno iteracijo
na matriki \(A - \sigma I\). Če je \(\sigma\) dober približek za lastno vrednosti, kot lastno vrednost dobimo \(\lambda - \sigma\). Tako smo izračunali najmanjšo lastno vrednost. Rezultat iteracije pa je njej pripadajoči lastni vektor.
\paragraph{QR razcep.} Matriko $A_k$ razcepimo na $A_k = Q_kR_k$, nato izračunamo naslednji člen $A_{k+1} = R_kQ_k$. Če so lastne vrednosti paroma različne, zaporedje konvergira proti Schurovi formi matrike (zgornje-trikotna matrika, ki ima na diagonali lastne vrednosti).
Metodo lahko izboljšamo tako, da matriko $A$ spremenimo v zgornjo Hessenbergovo matriko: Z diagonalnimi transformacijami dosežemo, da ima matrika elemente zgolj nad glavno diagonalo in na eni diagonalnii pod njo. S tem dobimo veliko ničel, ki zelo pospešijo proces.
Ko imamo matriko v tej obliki, $QR$ razcep konvergira precej hitreje. Če je $A$ simetrična, dobimo tridiagonalno matriko, na kateri lahko uporabimo kakšno hitrejšo metodo:
\paragraph{Sturmovo zaporedje.} Imamo simetrično tridiagonalno matriko: $$A = \begin{bmatrix}
    a_1 & b_1 & 0 & \dots & 0 \\
    b_1 & a_2 & b_2 & \dots & 0 \\
    0 & b_2 & a_3 &  & \vdots \\
    \vdots & \vdots &  &  & b_{n-1} \\
    0 & 0 & \dots & b_{n-1} & a_n 
\end{bmatrix}$$
Najprej predpostavimo, da so vsi $b_i \neq 0$. Če je ta predpostavka kršena, lahko matriko razbijemo na matrike
$$A = \begin{bmatrix}
    A_1 & 0 \\
    0 & A_2
\end{bmatrix}$$ in iščemo posebej lastne vrednosti $A_1$ in $A_2$.
Zdaj bodi $A_r$ vodilna podmatrika matrike $A$ velikosti $r \times r$ in $f_r(\lambda) = \det(T_r - \lambda I)$
Velja:
$$f_{r+1} = (a_{r+1} - \lambda)f_{r}(\lambda) - b^2_rf_{r-1}(\lambda)$$
Začetna pogoja sta $f_0(\lambda) = 1$ in $f_1(\lambda = a_1 - \lambda)$.
Na podlagi tega definiramo funkcijo $u$, za katero velja:
$$u(\lambda_k + \varepsilon) = k$$
$$u(\lambda_k - \varepsilon) = k-1$$
Zdaj lahko lastno vrednost iščemo z bisekcijo. Prednost tega je, da lahko najdemo katero koli lastno vrednost, ne da bi nam bilo treba izračunati še ostale.
\paragraph{Jacobijeva metoda.} Če matrika ni diagonalna, jo lahko množimo z rotacijami, dokler se vrednosti izven diagonale poljubno ne zmanjšajo (pod nek izbrani $\varepsilon$).
\end{document}